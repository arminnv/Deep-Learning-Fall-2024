# Deep Learning
This repository contains the coursework materials for the graduate course "Deep Learning", taught by Dr. Emad Fatemizadeh.

## Repository Structure

- `HW1/`: Introducation to Machine Learning
- `HW2/`: Optimization (incomplete due to TOEFL exam)
- `HW3/`: CNN Architectures and Transfer Learning
- `HW4/`: Time Series, NLP, LSTM, Attention, and LLMs
- `HW5/`: Deep Generative Models
- `Project/`: Real-Time Object Recognition
- 
• 
 • 
 • Shallow and Deep Neural networks as classifier and function approximator: Single layer 
perceptron (SLP), multilayer perceptron (MLP), error back propagation (EBP) algorithm, 
most important theorems.
 • Regularization, Normalization, and Optimization with emphasis on stochastic gradient 
descent (SGD) and its variations.
 • Convolutional Neural Networks (CNN): History, Foundations, Architecture, Learning.
 • Application of CNN in computer vision: Most important network (AlexNet, GoogleNet, 
VGGNet, ResNet , and state of art networks)
 Sharif University of Technology, School of Electrical Engineering
 7
 DEEP LEARNING, E. FATEMIZADEH, FALL 2024
Syllabus
 • Recurrent Neural Networks: RNN, LSTM, GRU, Transformers, applications 
in natural language and signal/image processing.
 • Unsupervised Learning: Auto Encoder (AE), Variational Auto Encoder 
(VAE), Conditional Variational Auto Encoder (CVAE)
 • Adversarial learning: Generative Adversarial Networks (GAN) and 
Conditional GAN (CGAN), mathematical foundation, architecture, 
applications, and most important networks (GAN, DCGAN, CycleGAN, 
WGAN, and state of art networks)
 • Generative Models (Diffusion)

## Syllabus

1. Introductions
   - An Introduction to Machine Learning Concepts, importance, applications, and examples.
2. Rapid Survey
   - Essential Mathematics for Machine Learning, Linear Algebra and Random Variables
3. Shallow and Deep Neural networks as classifier and function approximator
   - Single layer perceptron (SLP), multilayer perceptron (MLP), error back propagation (EBP) algorithm, most important theorems.
4. Regularization, Normalization, and Optimization
   - With emphasis on stochastic gradient descent (SGD) and its variations
5. Recurrent Neural Networks
   - RNN, LSTM, GRU, Transformers, applications in natural language and signal/image processing.
6. Unsupervised Learning
   - Auto Encoder (AE), Variational Auto Encoder (VAE), Conditional Variational Auto Encoder (CVAE)
7. Adversarial learning
   - Generative Adversarial Networks (GAN) and Conditional GAN (CGAN), mathematical foundation, architecture, applications, and most important networks (GAN, DCGAN, CycleGAN, WGAN, and state of art networks)
8. Generative Models (Diffusion)
    
For more details, please refer to the project report in the `Project/` directory.
