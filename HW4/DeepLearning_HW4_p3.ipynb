{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PfkDE6oamTY"
      },
      "source": [
        "\n",
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=10>\n",
        "    Deep Learning - HW4 <br>\n",
        "<font color=2565AE size=5>\n",
        "    Electrical Engineering Department <br>\n",
        "    winter 2024<br>\n",
        "<font color=3C99D size=5>\n",
        "    Practical Assignment 3 <br>\n",
        "<font color=696880 size=4>\n",
        "    Amirabbas Afzali\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSPsxozHamTa"
      },
      "source": [
        "# Personal Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4QHWhgpamTb"
      },
      "outputs": [],
      "source": [
        "# Set your student number\n",
        "student_number = '99105129'\n",
        "Name = 'Armin'\n",
        "Last_Name = 'Navardi'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_SujX54amTc"
      },
      "source": [
        "# Rules\n",
        "- Make sure that all of your cells can be run perfectly.\n",
        "- Try to minimize your use of ChatGPT (or any other AI assistant) as much as possible.\n",
        "- You must create a report for this task in PDF format and explain the main results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbhY1FdyamTd"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc_HtHEvamTd"
      },
      "source": [
        "## **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azdj8FrUamTe"
      },
      "source": [
        "Large Language Models (LLMs) are a class of deep learning models designed for processing and generating natural language. These models are trained using large amounts of textual data and utilize architectures based on transformers. Some of the applications of these models include text generation, machine translation, text summarization, question answering, and text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghhNET5aamTe"
      },
      "source": [
        "### *Encoder-Decoder LLMs*\n",
        "\n",
        "One of the common architectures in large language models is the Encoder-Decoder architecture. In this architecture, the encoder processes an input sequence and maps it to a latent space. Then, the decoder uses this latent space to generate an output sequence. Models like T5 [1] (Text-to-Text Transfer Transformer) use this architecture to perform various tasks. In T5, all tasks are expressed in a \"text-to-text\" format, meaning both input and output are text. This model has capabilities such as translation, summarization, and text classification. One of the advantages of the Encoder-Decoder architecture is that it allows the encoder to utilize information from both before and after a word to gain a more comprehensive understanding of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8_wJ5NUamTe"
      },
      "source": [
        "### *Decoder-only LLMs*\n",
        "\n",
        "Decoder-only models, such as GPT-2, GPT-3, and LLaMA [2], unlike the Encoder-Decoder architecture, only use the decoder part. These models use an autoregressive mode, meaning they predict the next token based on previous tokens. These models are highly efficient for text generation and have found widespread applications today.\n",
        "\n",
        "Advantages of Decoder-only Models\n",
        "\n",
        "- Efficiency: Decoder-only models are more efficient than Encoder-Decoder models due to the absence of an independent encoder. This makes them require fewer computational and memory resources.\n",
        "- Simplicity: Due to their autoregressive nature, these models can easily generate sequences in order.\n",
        "- Scalability: Due to their simpler architecture, these models can be scaled to much larger sizes.\n",
        "\n",
        "\n",
        "However, one of the drawbacks of these models is that they can only utilize information from tokens before the current token and cannot use tokens that come after for prediction. This limitation is significant in tasks like classification or translation, where a full understanding of the sequence is needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6cJtWyIamTe"
      },
      "source": [
        "## **Objective of the Exercise**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96rx9awwamTf"
      },
      "source": [
        "In this exercise, the goal is to convert a generative Decoder-only language model into an encoder and evaluate its performance on a binary sentiment classification task. The main aim is to modify the Decoder-only model so that it can function as an encoder and better handle tasks requiring bidirectional understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMQn5Pq-amTf"
      },
      "source": [
        "## **In this exercise, you should:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOstUdEfamTf"
      },
      "source": [
        "### In this exercise, you should:\n",
        "\n",
        "1. **Import a Decoder-only model** and load the weights of a pre-trained version of the model.\n",
        "2. **Generate several outputs from the model**, and include at most 10 sample outputs in your report for different inputs.  \n",
        "   You should also briefly explain the effects of key configurations in text generation, including:  \n",
        "   - `Temperature`\n",
        "   - `top_k`\n",
        "   - `top_p`\n",
        "   - `repetition_penalty`\n",
        "   - `num_beams`\n",
        "   - `no_repeat_ngram_size`\n",
        "3. **Load the SST-2 dataset**, which is part of the GLUE benchmark for sentiment classification.  \n",
        "   - Note that the model’s output depends on the number of input tokens.\n",
        "   - Apply necessary padding to the dataset after loading it to allow for parallel execution of the model.\n",
        "4. **Remove the model’s final layer**, which outputs to the size of the model’s dictionary.  \n",
        "   - Use the embedding vector of the first token (CLS token) for classification.\n",
        "5. As observed in the previous step, sometimes the embedding vector of the first token does not provide a good representation of the entire input text.  \n",
        "   - **Add a linear layer** with the same input and output dimensions on top of the encoder's output, and use the output of this linear layer (corresponding to the CLS token) for classification.  \n",
        "   - This step aggregates information of different tokens to get a comprehensive understanding of the input text.\n",
        "6. **Instead of the linear layer** in the previous section, use a **bidirectional attention layer** with a custom number of heads (preferably 12).\n",
        "7. **Repeat step 6** using **left-to-right unidirectional attention** and **right-to-left unidirectional attention**.\n",
        "8. **Load a pre-trained decoder** (preferably BERT-base) and report its **zero-shot performance** (i.e., without needing to train the model) on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXXIh7jVamTf"
      },
      "source": [
        "## **Evaluation:**\n",
        "\n",
        "In this exercise, for each of sections 4, 5, 6, 7, and 8, you need to plot the confusion matrix corresponding to the model's performance on the test data. Additionally, you should plot two separate graphs showing the training loss and the accuracy of the trained models, and compare them with each other, providing an appropriate analysis of your results. Also, note that high accuracy is not expected for sections 4 and 5, but the correctness of your code will be checked. However, for sections 6 and 7, higher accuracy (around 90%) is expected.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqgdbniGamTf"
      },
      "source": [
        "## **Let's go:**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install torchmetrics\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVR822HoPSpT",
        "outputId": "5c2cb74b-4f07-4579-a50d-5b50ef90a007"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1ChW3eJamTg"
      },
      "source": [
        "Load `gpt2` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSUp0Q8BamTg",
        "outputId": "76a1edf4-7246-4bc2-fdb5-a026d435db67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Model, BertForSequenceClassification\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vd9NoO6amTg"
      },
      "source": [
        "Load `sst-2` dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oEVsk51bamTg"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SST-2 dataset from Hugging Face\n",
        "dataset = load_dataset(\"glue\", \"sst2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0hkk1PKamTg"
      },
      "source": [
        "go ahead:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample outputs\n",
        "prompts = [\"The movie was\", \"The product is\", \"I think it will\"]\n",
        "model.eval()\n",
        "for prompt in prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_length=50, temperature=0.7, top_k=50, do_sample=True)\n",
        "    print(f\"Input: {prompt}\")\n",
        "    print(f\"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XxAJFOjPvOv",
        "outputId": "f51d6b8c-8507-4c0e-8b75-c7ab2acf4fe8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The movie was\n",
            "Output: The movie was released in English, but the English version has a few minor errors. In one scene one character is holding her hand and another is holding the other's back. The second scene depicts a man wearing a T-shirt and holding his head\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The product is\n",
            "Output: The product is ready to go to the customer's house.\n",
            "\n",
            "Warranty\n",
            "\n",
            "We guarantee all of our products in a safe and easy-to-use environment. We have a lifetime warranty on our products.\n",
            "\n",
            "Warranty\n",
            "\n",
            "Input: I think it will\n",
            "Output: I think it will be a good idea to keep it in the library for future use,\" said Michael L. Shulkin, executive director of the Institute for Advanced Study (IAS).\n",
            "\n",
            "The state's plan to build the first underground rail system\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertModel, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.nn import Linear, CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "sst2 = dataset.map(preprocess_function, batched=True)\n",
        "sst2.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "train_loader = DataLoader(sst2[\"train\"], batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(sst2[\"validation\"], batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "5UbkNqc2RtFa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a linear layer\n",
        "class GPT2WithLinearClassifier(torch.nn.Module):\n",
        "    def __init__(self, gpt2_model):\n",
        "        super(GPT2WithLinearClassifier, self).__init__()\n",
        "        self.gpt2 = gpt2_model\n",
        "\n",
        "        last_layer = nn.Linear(self.gpt2.config.hidden_size, self.gpt2.config.hidden_size, bias=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          last_layer.weight.copy_(torch.eye(self.gpt2.config.hidden_size))  # Identity matrix\n",
        "          last_layer.bias.zero_()  # Zero bias\n",
        "\n",
        "        self.gpt2.lm_head = last_layer\n",
        "\n",
        "        for param in self.gpt2.parameters():\n",
        "            param.reguires_grad = False\n",
        "\n",
        "        self.linear = torch.nn.Linear(self.gpt2.config.hidden_size, 1)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "        print(self.gpt2.config.hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        output = self.gpt2(input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = output['logits'][:, 0, :]\n",
        "\n",
        "        return torch.squeeze(self.activation(self.linear(cls_embedding)))\n"
      ],
      "metadata": {
        "id": "M4ZJaXK4htwe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import torchmetrics\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def train_one_epoch(model, train_loader, loss_fn, optimizer, metric, epoch, device):\n",
        "  model.train()\n",
        "  loss_train = AverageMeter()\n",
        "  metric.reset()\n",
        "\n",
        "  with tqdm.tqdm(train_loader, unit='batch') as tepoch:\n",
        "    for batch in tepoch:\n",
        "      if epoch:\n",
        "        tepoch.set_description(f'Epoch {epoch}')\n",
        "\n",
        "      inputs = batch['input_ids'].to(device)\n",
        "      targets = batch['label'].float().to(device)\n",
        "      mask = batch['attention_mask'].to(device)\n",
        "\n",
        "      outputs = model(inputs, mask)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss_train.update(loss.item(), n=len(targets))\n",
        "      metric.update((outputs > 0.5).float(), targets)\n",
        "\n",
        "      tepoch.set_postfix(loss=loss_train.avg, metric=metric.compute().item())\n",
        "\n",
        "  return model, loss_train.avg, metric.compute().item()\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, loss_fn, metric, device):\n",
        "    model.eval()\n",
        "    loss_eval = AverageMeter()\n",
        "    metric.reset()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "      for batch in test_loader:\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "        targets = batch['label'].float().to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss_eval.update(loss.item(), n=len(targets))\n",
        "\n",
        "        metric((outputs > 0.5).float(), targets)\n",
        "\n",
        "    return loss_eval.avg, metric.compute().item()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TKnIADGNiP43"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = GPT2WithLinearClassifier(model).to(device)\n",
        "loss_fn = nn.BCELoss()\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
        "optimizer = torch.optim.SGD(linear_model.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "\n",
        "loss_train_hist = []\n",
        "loss_valid_hist = []\n",
        "\n",
        "metric_train_hist = []\n",
        "metric_valid_hist = []\n",
        "\n",
        "best_loss_valid = torch.inf\n",
        "epoch_counter = 0\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  linear_model, loss_train, metric_train = train_one_epoch(linear_model, train_loader, loss_fn, optimizer, metric, epoch, device)\n",
        "  loss_valid, metric_valid = evaluate(linear_model, val_loader, loss_fn, metric, device)\n",
        "\n",
        "  loss_train_hist.append(loss_train)\n",
        "  loss_valid_hist.append(loss_valid)\n",
        "\n",
        "  metric_train_hist.append(metric_train)\n",
        "  metric_valid_hist.append(metric_valid)\n",
        "\n",
        "  if loss_valid < best_loss_valid:\n",
        "    torch.save(linear_model, f'model.pt')\n",
        "    best_loss_valid = loss_valid\n",
        "    print('Model Saved!')\n",
        "\n",
        "  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n",
        "  print()\n",
        "  epoch_counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "z7znjr5ZpJSh",
        "outputId": "9343db9b-5a93-47f4-e2aa-dbe7f175e596"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 84/4210 [00:46<38:05,  1.80batch/s, loss=0.732, metric=0.549]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f45bc78cc6a2>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mloss_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d2941e72d64f>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, loss_fn, optimizer, metric, epoch, device)\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0mtepoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mset_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m                                  for key in postfix.keys())\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2_Linear(torch.nn.Module):\n",
        "    def __init__(self, gpt2_model):\n",
        "        super(GPT2_Linear, self).__init__()\n",
        "        self.gpt2 = gpt2_model\n",
        "\n",
        "        last_layer = nn.Linear(self.gpt2.config.hidden_size, self.gpt2.config.hidden_size, bias=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          last_layer.weight.copy_(torch.eye(self.gpt2.config.hidden_size))  # Identity matrix\n",
        "          last_layer.bias.zero_()  # Zero bias\n",
        "\n",
        "        self.gpt2.lm_head = last_layer\n",
        "\n",
        "        for param in self.gpt2.parameters():\n",
        "            param.reguires_grad = False\n",
        "ِ\n",
        "        self.linear = nn.Sequential(\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(self.gpt2.config.hidden_size*128, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 1),\n",
        "                    nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        output = self.gpt2(input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = output['logits']\n",
        "\n",
        "        return torch.squeeze(self.linear(cls_embedding))"
      ],
      "metadata": {
        "id": "9j4aXHJhvcND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = GPT2_Linear(model).to(device)\n",
        "loss_fn = nn.BCELoss()\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
        "optimizer = torch.optim.SGD(linear_model.parameters(), lr=1e-3, momentum=0.9)\n",
        "\n",
        "\n",
        "loss_train_hist = []\n",
        "loss_valid_hist = []\n",
        "\n",
        "metric_train_hist = []\n",
        "metric_valid_hist = []\n",
        "\n",
        "best_loss_valid = torch.inf\n",
        "epoch_counter = 0\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  linear_model, loss_train, metric_train = train_one_epoch(linear_model, train_loader, loss_fn, optimizer, metric, epoch, device)\n",
        "  loss_valid, metric_valid = evaluate(linear_model, val_loader, loss_fn, metric, device)\n",
        "\n",
        "  loss_train_hist.append(loss_train)\n",
        "  loss_valid_hist.append(loss_valid)\n",
        "\n",
        "  metric_train_hist.append(metric_train)\n",
        "  metric_valid_hist.append(metric_valid)\n",
        "\n",
        "  if loss_valid < best_loss_valid:\n",
        "    torch.save(linear_model, f'model.pt')\n",
        "    best_loss_valid = loss_valid\n",
        "    print('Model Saved!')\n",
        "\n",
        "  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n",
        "  print()\n",
        "  epoch_counter += 1"
      ],
      "metadata": {
        "id": "Xu4KO0RCwlpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6TfrI1NkamTg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class BidirectionalAttentionClassifier(torch.nn.Module):\n",
        "    def __init__(self, gpt2_model, num_heads=12):\n",
        "        super().__init__()\n",
        "        self.gpt2 = gpt2_model\n",
        "\n",
        "        for param in self.gpt2.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "        self.multihead_attn = torch.nn.MultiheadAttention(self.gpt2.config.hidden_size, num_heads)\n",
        "        self.linear = nn.Sequential(\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(768, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 1),\n",
        "                    nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        output = self.gpt2(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        attn_output, _ = self.multihead_attn(output.last_hidden_state[:, 0, :].unsqueeze(0),\n",
        "                                             output.last_hidden_state.transpose(0, 1),\n",
        "                                             output.last_hidden_state.transpose(0, 1))\n",
        "\n",
        "        return torch.squeeze(self.linear(attn_output.squeeze(0)))\n",
        "\n",
        "\n",
        "\n",
        "# Repeat using left-to-right unidirectional attention and right-to-left unidirectional attention\n",
        "class UnidirectionalAttentionClassifier(torch.nn.Module):\n",
        "    def __init__(self, gpt2_model, direction=\"left-to-right\"):\n",
        "        super().__init__()\n",
        "        self.gpt2 = gpt2_model\n",
        "\n",
        "        for param in self.gpt2.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "        self.direction = direction\n",
        "        self.attn_layer = torch.nn.MultiheadAttention(self.gpt2.config.hidden_size, num_heads=1, add_bias_kv=True)\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(768, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 1),\n",
        "                    nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.mask = torch.triu(torch.ones((128, 128), device='cuda'), diagonal=1).bool()\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        output = self.gpt2(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        if self.direction == \"right-to-left\":\n",
        "            reversed_output = torch.flip(output.last_hidden_state, dims=[1])\n",
        "            attn_output, _ = self.attn_layer(output.last_hidden_state[:, 0, :].unsqueeze(0),\n",
        "                                             reversed_output.transpose(0, 1),\n",
        "                                             reversed_output.transpose(0, 1), attn_mask=self.mask)\n",
        "        else:  # Default left-to-right\n",
        "            attn_output, _ = self.attn_layer(output.last_hidden_state[:, 0, :].unsqueeze(0),\n",
        "                                             output.last_hidden_state.transpose(0, 1),\n",
        "                                             output.last_hidden_state.transpose(0, 1), attn_mask=self.mask)\n",
        "        return torch.squeeze(self.linear(attn_output.squeeze(0)))\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Model\n",
        "import torch.nn as nn\n",
        "device = 'cuda'\n",
        "model = GPT2Model.from_pretrained(\"gpt2\").to(device)"
      ],
      "metadata": {
        "id": "bp67lIlSQ6mU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bidirectional_model = BidirectionalAttentionClassifier(model).to(device)\n",
        "loss_fn = nn.BCELoss()\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
        "#optimizer = torch.optim.SGD(bidirectional_model.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(bidirectional_model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "loss_train_hist = []\n",
        "loss_valid_hist = []\n",
        "\n",
        "metric_train_hist = []\n",
        "metric_valid_hist = []\n",
        "\n",
        "best_loss_valid = torch.inf\n",
        "epoch_counter = 0\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  bidirectional_model, loss_train, metric_train = train_one_epoch(bidirectional_model, train_loader, loss_fn, optimizer, metric, epoch, device)\n",
        "  loss_valid, metric_valid = evaluate(bidirectional_model, val_loader, loss_fn, metric, device)\n",
        "\n",
        "  loss_train_hist.append(loss_train)\n",
        "  loss_valid_hist.append(loss_valid)\n",
        "\n",
        "  metric_train_hist.append(metric_train)\n",
        "  metric_valid_hist.append(metric_valid)\n",
        "\n",
        "  if loss_valid < best_loss_valid:\n",
        "    torch.save(bidirectional_model, f'model.pt')\n",
        "    best_loss_valid = loss_valid\n",
        "    print('Model Saved!')\n",
        "\n",
        "  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n",
        "  print()\n",
        "  epoch_counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGcuJ1wzmuN8",
        "outputId": "3ed39de7-749e-40ad-8eee-d1bbe03bb36b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4210/4210 [14:59<00:00,  4.68batch/s, loss=0.509, metric=0.74]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 0.381, Metric = 0.8268\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 4210/4210 [14:58<00:00,  4.68batch/s, loss=0.437, metric=0.798]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Saved!\n",
            "Valid: Loss = 0.3756, Metric = 0.8429\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ltr_model = UnidirectionalAttentionClassifier(model, direction=\"left-to-right\").to(device)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
        "#optimizer = torch.optim.SGD(ltr_model.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(ltr_model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "loss_train_hist = []\n",
        "loss_valid_hist = []\n",
        "\n",
        "metric_train_hist = []\n",
        "metric_valid_hist = []\n",
        "\n",
        "best_loss_valid = torch.inf\n",
        "epoch_counter = 0\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  ltr_model, loss_train, metric_train = train_one_epoch(ltr_model, train_loader, loss_fn, optimizer, metric, epoch, device)\n",
        "  loss_valid, metric_valid = evaluate(ltr_model, val_loader, loss_fn, metric, device)\n",
        "\n",
        "  loss_train_hist.append(loss_train)\n",
        "  loss_valid_hist.append(loss_valid)\n",
        "\n",
        "  metric_train_hist.append(metric_train)\n",
        "  metric_valid_hist.append(metric_valid)\n",
        "\n",
        "  if loss_valid < best_loss_valid:\n",
        "    torch.save(ltr_model, f'ltr_model.pt')\n",
        "    best_loss_valid = loss_valid\n",
        "    print('Model Saved!')\n",
        "\n",
        "  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n",
        "  print()\n",
        "  epoch_counter += 1"
      ],
      "metadata": {
        "id": "6drOIccztYK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rtl_model = UnidirectionalAttentionClassifier(model, direction=\"right-to-left\").to(device)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
        "#optimizer = torch.optim.SGD(rtl_model.parameters(), lr=1e-3, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(rtl_model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "loss_train_hist = []\n",
        "loss_valid_hist = []\n",
        "\n",
        "metric_train_hist = []\n",
        "metric_valid_hist = []\n",
        "\n",
        "best_loss_valid = torch.inf\n",
        "epoch_counter = 0\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  rtl_model, loss_train, metric_train = train_one_epoch(rtl_model, train_loader, loss_fn, optimizer, metric, epoch, device)\n",
        "  loss_valid, metric_valid = evaluate(rtl_model, val_loader, loss_fn, metric, device)\n",
        "\n",
        "  loss_train_hist.append(loss_train)\n",
        "  loss_valid_hist.append(loss_valid)\n",
        "\n",
        "  metric_train_hist.append(metric_train)\n",
        "  metric_valid_hist.append(metric_valid)\n",
        "\n",
        "  if loss_valid < best_loss_valid:\n",
        "    torch.save(rtl_model, f'rtl_model.pt')\n",
        "    best_loss_valid = loss_valid\n",
        "    print('Model Saved!')\n",
        "\n",
        "  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n",
        "  print()\n",
        "  epoch_counter += 1"
      ],
      "metadata": {
        "id": "eSp13qFNtZt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT decoder and report zero-shot performance\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "bert_model.eval()\n",
        "accuracy = 0\n",
        "length = 0\n",
        "\n",
        "with torch.inference_mode():\n",
        "  for item in dataset['test']:\n",
        "    try:\n",
        "      inputs = tokenizer(item['sentence'], return_tensors=\"pt\")\n",
        "      outputs = bert_model(**inputs)\n",
        "      length += 1\n",
        "    except:\n",
        "      continue\n",
        "    if length>10:\n",
        "      break\n",
        "\n",
        "    outputs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0][0]\n",
        "    print(outputs)\n",
        "    targets = item['label']\n",
        "    pred = outputs < 0.5\n",
        "    accuracy += (pred ==targets)\n",
        "    print(accuracy/length)\n",
        "\n",
        "print(f\"Zero-shot accuracy: {accuracy/length}\")"
      ],
      "metadata": {
        "id": "OFI2G_RUJ30t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu3P9JnkamTg"
      },
      "source": [
        "---------\n",
        "### References\n",
        "\n",
        "[1] Raffel, Colin, Noam Shazeer, Adam Roberts, et al. (2020). *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*. [Link to paper](https://arxiv.org/abs/1910.10683)\n",
        "\n",
        "[2] Touvron, Hugo, et al. (2023). *LLaMA 2: Open Foundation and Fine-Tuned Chat Models*. [Link to paper](https://arxiv.org/abs/2307.09288)\n",
        "\n",
        "<span style=\"color:yellow;\">*For further reading on this field of research, you can refer to the following papers:*</span>\n",
        "\n",
        "[3] BehnamGhader, Adlakha, et al. (2024). *LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders*. [Link to paper](https://arxiv.org/abs/2404.05961)\n",
        "\n",
        "[4] Gao, Tianyu, et al. (2021). *SimCSE: Simple Contrastive Learning of Sentence Embeddings*. [Link to paper](https://arxiv.org/abs/2104.08821)\n",
        "\n",
        "[5] Lee, et al. (2023). *NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models*. [Link to paper](https://arxiv.org/abs/2405.17428)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ZCLBc1amTh"
      },
      "source": [
        "# **Best regards.**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "550b73d17028e65cfbd266e0c945d7274f18a7a366e249c5ab11fc4eb0cd2459"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}